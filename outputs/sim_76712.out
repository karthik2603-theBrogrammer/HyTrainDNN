===== Running HyTrainDNN Framework =====
[2025-11-04 12:05:37,317] [INFO] [real_accelerator.py:260:get_accelerator] Setting ds_accelerator to cuda (auto detect)
Warning: The cache directory for DeepSpeed Triton autotune, /scratch/karthick/skipar/temp_home/.triton/autotune, appears to be on an NFS system. While this is generally acceptable, if you experience slowdowns or hanging when DeepSpeed exits, it is recommended to set the TRITON_CACHE_DIR environment variable to a non-NFS path.
[2025-11-04 12:05:38,861] [INFO] [logging.py:107:log_dist] [Rank -1] [TorchCheckpointEngine] Initialized with serialization = False
===== Running HYTRAIN Framework =====
=====  framework: hytrain
=====  lr: 2e-05
=====  batch_size: 8
=====  context_length: 2048
=====  epochs: 3
=====  steps: 1000
=====  warmup_steps: 100
=====  use_mixed_precision: True
=====  use_gradient_checkpointing: True
=====  model: gpt2
=====  model_size: gpt2_9b
=====  dataset: c4
=====  model_checkpoint_path: None
=====  opt_checkpoint_path: None
=====  metadata_checkpoint_path: None
=====  models_dir: /scratch/karthick/skipar/Comprehensive-and-Integrated-Framework-for-ML-DL-Applications/Heterogeneous-Training/models
=====  seed: 42
=====  alpha: 0.083
=====  checkpoint_interval: 4000
=====  shuffle: True
=====  shuffle_buffer_size: 10000
==================================================
âœ… Set deterministic seed: 42
=== Context Length:  2048
Total parameters in model: 8.91B
GPTLMModel(
  (model): GPT2LMHeadModel(
    (transformer): GPT2Model(
      (wte): Embedding(50257, 4480)
      (wpe): Embedding(2048, 4480)
      (drop): Dropout(p=0.1, inplace=False)
      (h): ModuleList(
        (0-35): 36 x GPT2Block(
          (ln_1): LayerNorm((4480,), eps=1e-05, elementwise_affine=True)
          (attn): GPT2Attention(
            (c_attn): Conv1D(nf=13440, nx=4480)
            (c_proj): Conv1D(nf=4480, nx=4480)
            (attn_dropout): Dropout(p=0.1, inplace=False)
            (resid_dropout): Dropout(p=0.1, inplace=False)
          )
          (ln_2): LayerNorm((4480,), eps=1e-05, elementwise_affine=True)
          (mlp): GPT2MLP(
            (c_fc): Conv1D(nf=17920, nx=4480)
            (c_proj): Conv1D(nf=4480, nx=17920)
            (act): NewGELUActivation()
            (dropout): Dropout(p=0.1, inplace=False)
          )
        )
      )
      (ln_f): LayerNorm((4480,), eps=1e-05, elementwise_affine=True)
    )
    (lm_head): Linear(in_features=4480, out_features=50257, bias=False)
  )
)
[extension] Loading the JIT-built cpu_adam_x86 kernel during runtime now
[extension] Time taken to load cpu_adam_x86 op: 39.868547439575195 seconds
total layers:  39
=== Total Layers in the model: 39
=== Total parameter groups: 219
optimizer initialized
=== Timestamp:  20251104_120750
âœ… Created HYTRAIN trainer with consistent configuration
ðŸš€ Starting HYTRAIN training...
Starting training threads...
CPU update thread started
Communication thread started
Loaded GPT2 Tokenizer.
Using c4 dataset.
== Tokenizer vocab size:  50257
  0%|          | 0/100 [00:00<?, ?it/s]